{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Recuperação da Informação</b></h1>\n",
    "\n",
    "<h3><b>Recuperação de Informação na Web 2020</b></h3>\n",
    "\n",
    "<h3><b>Máquina de Busca para a Wikipédia</b></h3>\n",
    "<br>\n",
    "Alunos:<ul>Antônio Sousa</ul><ul>Mariana Bulgarelli Alves dos Santos</ul><ul>Nathã Paulino</ul>\n",
    "\n",
    "<h4><b>Indexação</b></h4>\n",
    "\n",
    "<p>Máquinas de busca de tamanho médio fazem uso de duas técnicas principais, pesquisa online e indexada. A pesquisa online é adequada para coleções voláteis, pequenas ou onde “a sobrecarga do espaço de índice não pode ser fornecida”[1]. Uma pesquisa de texto sequencial ou online varre o texto não processado em busca de ocorrências de um padrão específico [1].\n",
    "Uma estrutura de índices, por sua vez, é uma estrutura de dados que visa acelerar a pesquisa e é ideal para uma coleção grande.</p>\n",
    "\n",
    "<h4><b>O que é um índice</b></h4>\n",
    "\n",
    "<p>Índices são estruturas de dados que visam aumentar a velocidade da recuperação da informação. Os índices “são baseados em estruturas de dados árvores (tree) ou estruturas de dados hash” [2]. Um índice pode ser gerado a partir de qualquer campo de um arquivo e índices podem ser gerados a partir destes índices.\n",
    "    \n",
    "“Para encontrar um registro ou registros [...] a partir de um campo de indexação, é necessário inicialmente acessar o índice, que aponta para um ou mais blocos do arquivo em que os registros requeridos estão localizados” [2].\n",
    "    \n",
    "Índices baseados em arquivos ordenados (índices de nível único - primário, secundário e clustering) e estruturas de dados de árvores (índices multiníveis, árvores-B) são comuns.\n",
    "    \n",
    "O índice armazena cada valor com uma lista de ponteiros para onde contém registros. “Os valores no índice são ordenados de forma que possamos realizar uma busca binária. O arquivo de índice é muito menor que o arquivo de dados, assim, a busca do índice por meio da busca binária é razoavelmente eficiente” [2]. Em índices multiníveis não há necessidade de realizar a busca binária. \n",
    "\n",
    "Um índice invertido (arquivo invertido) é composto por dois elementos: vocabulário (todas as palavras diferentes do texto) e  ocorrências (lista de todas as posições de texto onde a palavra aparece). As posições facilitam o acesso direto às posições de texto onde se encontram as palavras/caracteres [1]. Para reduzir o custo da operação, pode-se indexar por blocos. Um arquivo invertido, segundo BAEZA-YATES e RIBEIRO-NETO é composto normalmente por um vetor com todas as palavras diferentes de um conjunto de textos (vocabulário) ou composto por cada palavra do vocabuláro. \n",
    "    \n",
    "Para o projeto em questão, foi realizada a indexação por documento, ou seja, o índice contém uma lista com ponteiros para os documentos que possuem uma certa palavra-chave. Contém o vocabulário, o número de documentos associados com as palavras e uma lista de ocorrências da palavra nos documentos. Cada entrada da lista representa o número do documento que apresenta a palavra e o número de ocorrências.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Estrutura de índice implementada</b></h3>\n",
    "\n",
    "<p>Foi utilizado um índice invertido. A explicação das classes, métodos e a definição encontram-se a seguir. A estruturação dá-se com a estrutura de dados em árvore, no caso, árvore B (generalização da árvore de pesquisa binária). Uma árvore B permite inserção, remoção e busca com complexidade de tempo logarítmica.Em cada nó são armazenadas mais do que uma chave, ordenadas, e cujo endereçamento é realizado em memória secundária.</p>\n",
    "\n",
    "<h4><b>Structure</b></h4>\n",
    "\n",
    "Nesta etapa implementou-se dois itens principais, o `HashIndex` (índice simples em memória principal) e o `FileIndex` (índice com as ocorrências em memória secundária). A utilização de um índce em memória secundária viabiliza a indexação de mais páginas.\n",
    "\n",
    "\n",
    "<h4>Classe Index:</h4>\n",
    "\n",
    "Esta classe inicializa no construtor os atributos de `dict_index` (dicionário onde a chave é a string do termo indexação e os valores podem der de diferentes tipos) e `set_documents` (conjunto de ids de documentos) além de possuir os métodos abstratos `get_term_id`, `create_index_entry`, `add_index_occur`, `get_occurrence_list` e `ocument_count_with_term`. Estes métodos foram implementados nas classes filhas de Index.\n",
    "\n",
    "O método `index` recebe como parâmetros term, doc_id e term_freq para que possa indexar um termo com sua frequncia e documento no índice. Se o term não existir no índice, obtem-se o próximo term_id, que está armazenado de forma sequencial. Se estiver, o método `get_term_id` é chamado para obtê-lo. Em seguida, adiciona-se o id do documento na lista de documentos e o método para adicionar uma nova ocorrência de um termo é chamado `add_index_occur`.\n",
    "O atributo calculado `vocabulary` percorre o dicionário de termos para retornar a lista do vocabulário indexado, enquanto que `document_count`retorna a quantidade de documentos existentes. \n",
    "\n",
    "<h4>Classe HashIndex:</h4>\n",
    "\n",
    "Esta classe cria o indice e suas ocorrencias em memória principal. A classe possui o método `create_index_entry` que recebe um term_id e cria uma nova entrada no índice (utilizando ou não o id recebido como parâmetro). O método `add_index_occur` adiciona uma nova ocorrência na entrada do índice utilizando entry_dic_index, doc_id, term_id e term_freq passados como parâmetro. Os demais métodos da classe são responsáveis por retornar a lista de ocorrências de um determinado termo passado como parâmetro (`get_occurrence_list`: se o termo não existe, retorna vazio) e por retornar a quantidade de documentos que possuem um determinado termo (`document_count_with_term`). Neste último é realizada a verificação do tamanho da lista de ocorrência do termo. Se o retorno for uma lista vazia, não ha documentos relacionados ao termo. Se não, retorna a quantidade de documentos, que é equivalente ao tamanho da lista.\n",
    "\n",
    "    \n",
    "<h4>Classe TermOccurrence:</h4>\n",
    "\n",
    "A classe recebe em seu construtor doc_id,term_id e term_freq. Possui o método `write` que recebe um arquivo e salva na ordem adequada (na forma de bytes usando `to_bytes`) neste arquivo o term_id, doc_id e term_freq. Além disso, a classe possui alguns métodos de comparação para ordenar as ocorrências, a saber: `__eq__` e `__lt__`. `__eq__` retorna true se um objeto é igual ao outro pela comparação do id do termo dela e o id do documento (ambos precisam ser iguais). Retorna False caso contrário. O método `__lt__` retorna verdadeiro se o objeto corrrente self é menor do que o objeto passado como parametro. \n",
    "\n",
    "\n",
    "<h4>Classe TermFilePosition:</h4>\n",
    "\n",
    "A classe recebe em seu construtor os parâmetros term_id, term_file_start_pos (posição inicial em bytesde um term_id no arquivo), doc_count_with_term (quantidade de ocorrencias do termo - None por default).\n",
    "\n",
    "    \n",
    "<h4>Classe FileIndex</h4>\n",
    "\n",
    "Esta classe armazena as ocorrências em arquivo. Esta classe herda da classe Index seus atributos e inicializa em seu construtor `lst_occurrences_tmp`(`[]`), `idx_file_counter`(0) e `str_idx_file_name`(None). O método `create_index_entry` recebe como parãmetro um term_id e retorna um objeto TermFilePosition deste term_id, enquanto `get_term_id` recebe um termo e retorna o term_id deste termo no dict_index. `add_index_occur` cria e acrescenta mais um objeto TermOccurrence em lst_occurrences_tmp.\n",
    "\n",
    "Os métodos `next_from_list` e `next_from_file` servem para obter, respectivamente os próximos objetos TermOccurrence (o menor) na lista `lst_occurrences_tmp` e no arquivo para que possam ser comparados e, então armazenados em um outro arquivo na ordem adequada. Em `next_from_list` retorna o primeiro termo (removido da lista) posto que a mesma está ordenada. Se está vazia, retorna None. Para o caso de `next_from_file` inicialmente, realizamos o tratamento de nome de arquivo igual a None (aplicado na primeira iteração do save_tmp_occurrences em nossa implementação). Em seguida, obtemos doc_id, term_id e term_freq como bytes do arquivo e passamos para int para compor o objeto TermOccurrence retornado.\n",
    "\n",
    "O método `save_tmp_occurrences` salva lst_occurrences_tmp (lista de ocorrencias que serão armazenadas em arquivo) em arquivo de forma ordenada. Inicialmente gera-se o padrão de nomenclatura para o próximo documento (criando `str_idx_new_file_name` - arquivo indice atual) e ordena-se `lst_occurrences_tmp`. Realiza-se a abertura do arquivo com with open e obtêm-se os próximos termos da lista e do arquivo antigo para comparação utilizando `next_from_list` e `next_from_file`. O menor TermOcurrence é escrito no novo arquivo e o próximo TermOcurrence é obtido da mesma fonte daquele que acabou de ser escrito no novo arquivo. Após este processo, o arquivo antigo é apagado e atualiza-se o nome do arquivo corrente. `idx_file_counter` (serve para definir o nome do arquivo do índice) sofre acréscimo e `lst_occurrences_tmp` recebe vazio.\n",
    "\n",
    "O método `finish_indexing` organiza o dic_index atualizando o term_file_start_pos (posição inicial) e doc_count_with_term (quantidade de documentos de cada termo) para os valores corretos. Para tal navega-se nas ocorrências para atualizar cada termo em dic_ids_por_termo apropriadamente. Por atualizar, entende-se colocar um valor diferente do 0 colocado por default.\n",
    "\n",
    "`get_occurrence_list` faz a busca no dicionario e pega os objetos TermFilePosition. Se o termo recebido como parâmetro não está em dic_index, retorna occurrence_list. Do contrário, realiza-se a leitura de str_idx_file_name e posiciona-se a leitura do arquivo no início da listagem do termo buscado. em seguida, coleta-se o primeiro TermOcurrence respectivo ao term buscado e percorre-se o arquivo em busca de novas ocorrências de term. Como o term_id encontra-se agrupado no arquivo, a partir do momento que lemos outro, sai do while para retornar occurrence_list.\n",
    "\n",
    "\n",
    "<h4><b>Indexador de HTML</b></h4>\n",
    "\n",
    "O indexador executará a etapa de preprocessamento do conteúdo html e depois efetuará a indexação. O preprocessamento ocorre na classe Cleaner. A Classe HTMLIndexer realiza a indexação.\n",
    "\n",
    "<h4>Classe Cleaner:</h4>\n",
    "\n",
    "A classe recebe em seu construtor stop_words_file (arquivo de stopwords),language, perform_stop_words_removal, perform_accents_removal e perform_stemming como parâmetros.\n",
    "\n",
    "As flags (que definem se haverá processamento opcional) `perform_stop_words_removal`, `perform_accents_removal`, `perform_stemming`, são inicializadas com `perform_stop_words_removal`, `perform_accents_removal` e `perform_stemming`, respectivamente, todos boolean.\n",
    "\n",
    "Um steammer é gerado com base na linguagem utilizando a biblioteca SnowballSteammer, o arquivo de stopwords passado como parâmetro é lido e armazenado em um set (`set_stop_words`) e são inicializados `in_table`,`out_table`,`accents_translation_table` e o `set_punctuation`. Esta classe atua antes da indexação e remove tudo o que não é interessante de se indexar.\n",
    "\n",
    "O método `html_to_plain_text` recebe um documento html, parseia com o beautifulSoup e extrai o texto que é retornado. Com relação às stopwords, os métodos `is_stop_word` (verifica se um termo é ou não stopword comparando com o `set_stop_words`) e `read_stop_words` (recebe o aruivo com as stopwords, lê e adiciona em `set_stop_words`) atuam.\n",
    "\n",
    "O método `word_stem` efetua o stemming, ou seja, remove prefixos e sufixos das palavras, restando apenas a raiz da mesma por meio do atributo instanciado no construtor da classe (stemmer - classe SnowballStemmer da API NLTK). `remove_accents` recebe um termo como parâmetro e remove os acentos por meio de `in_table` e `out_table`. Usamos ord e chr para tal, sendo que o primeiro busca a referência de uma letra na tabela ASCII em python e o segundo recebe a referência retornada em ASCII e converte pra char para então retornar um term sem acentos.\n",
    "\n",
    "O método `preprocess_word` recebe um termo e verifica se é este é válido de ser indexada. Inicialmente verifica se não é pontuação ou alguma stopword, chamando os métodos responsáveis por tal. Além disso, todas as palavras recebidas como parâmetro por este método são passadas para minúsculas e os acentos são removidos, caso necessário. Por fim, é realizado o stemming.\n",
    "\n",
    "<h4>Classe HTMLIndexer</h4>\n",
    "\n",
    "Esta classe realiza a indexação de fato e recebe como parâmetro um objeto index que inicializa o atributo de mesmo nome. A classe HTMLIndexer instancia um objeto da classe Cleaner. O método `text_word_count` recebe o testo html tratado e retorna um dicionário cujas chaves são os termos e os valores são as frequências.\n",
    "\n",
    "O método `index_text` recebe um doc_id e o testo desse documento e converte o html para texto simples com o método `html_to_plain_text` da classe Cleaner. Em seguida converte o texto em um dicionário de ocorrências com `text_word_count` para indexar cada palavra como um objeto TermOccurrence.\n",
    "\n",
    "`index_text_dir` efetua a indexação de um diretorio com subdiretorios, navegando nesses subdiretórios e indexando os arquivos neles contidos. O método recebe um path como parâmetro, monta o path completo dos subdiretórios e percorre seu conteúdo para montar o path completo dos arquivos. A partir daí, os arquivos são lidos e os doc_id e texto coletados para serem finalmente indexados pelo método index_text.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para resolver esse problema, uma solução é mantermos o vocabulário em memória principal e as ocorrências em memória secundária. Assim, teriamos o mesmo atributo dic_index na classe Index. Porém, cada entrada (termo) referenciará as ocorrencias em arquivo. Utilizando exemplo da atividade 3 neste contexto, no final da indexação, o dic_index deve ficar da seguinte forma:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Link para download do arquivo com índice e vocabulário e instruções para abrir</b></h3>\n",
    " \n",
    "<br> https://drive.google.com/drive/folders/1pzMgBVHoRnSk0pHChQq4I8FRSzfwc5Qv?usp=sharing </br>\n",
    "\n",
    "Os arquivos se encontram em formato de leitura para humanos, portanto é só abrir os arquivos em Indexadores para ter acesso aos indices em JSON e ambos os vocabulários estão em formato txt padrão!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Principais desafios e soluções</b></h3>\n",
    "\n",
    "<p>Um dos principais desafios foi salvar o index e vocabulário em arquivo. Com relação ao vocabulário, algumas ocorrências estranhas, que não foram capturadas pelo stopwords causaram problemas, como os emojis. Para salvar o index em um arquivo foi utilizada uma estrutura JSON e através dela estendeu-se o encoder para deixar o indexador em uma estrutura serializável.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Qual é a vantagem/desvantagem das suas soluções sob as outras alternativas? </b></h3>\n",
    "<p>(por exemplo, uso do índice em memória principal x ocorrência de termos em memória secundária)</p>\n",
    "\n",
    "A memória principal, é não volátil e é endereçada diretamente pelo processador. Assim apresenta baixo tempo de acesso, mas armazena um volume relativamente pequeno de informação. A memória secundária armazena os dados permanentemente no sistema, possui um endereçamento indireto, mas o acesso é mais lento que na memória primária.\n",
    "\n",
    "Assim, a construção de índice usando apenas memória principal é simples de se implementar e eficiente em termos de tempo de execução, ou seja, o uso de índices em memória principal apresentam uma resposta mais rápida. Porém, quando precisa-se indexar um número muito grande de páginas, torna-se inviável armazenar tudo em memória principal.\n",
    "\n",
    "A maior desvantagem da utilização da memória secundária é a questão do tempo de acesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>O que você melhoraria no seu código para diminuir o consumo de memoria ou deixá-lo mais eficiente?</b></h3>\n",
    "\n",
    "\n",
    "Tentaríamos paralelizar o código para deixá-lo mais eficiente. Para diminuir o consumo de memória, a solução que achamos interessante é passar toda a estrutura do índice para memória secundária. no entanto, ficaria mais lento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Quanto MB de ram cada solução de índice gastou?</b></h3>\n",
    "\n",
    "A solução que utiliza o HashIndex gastou cerca de 782 MB e a solução utilizando o FileIndex gasta muito menos memória -300 MB (Isso quer dizer que houve uma liberação de memória comparado aos valores anteriores), logo estima-se que foi utilizado 482 MB. Essa diferença se dá pelo fato de uso de memória secundária na segunda solução, porém ela tem um preço que é o tempo de indexação dos mesmos documentos.\n",
    "\n",
    "<h3><b>Em quanto tempo foi realizado a indexação? Qual foi a média por documento?</b></h3>\n",
    "\n",
    "A indexação utilizando a estrutura de HashIndex - gerenciamento do índice em memória - gastou cerca de 42 minutos (0,041 segundos por arquivo) e a estruturação utilizando o FileIndex - salvamento dos índices em memória secundária - gastou cerca de 106 minutos (0,102 segundos por arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Explique o funcionamento da técnica de stemming adotada</b></h3>\n",
    "\n",
    "<p>Stemming é uma técnica responsável por retirar prefixos e sufixos (já pré estabelecidos para cada linguagem específica) com o intuito de buscar palavras que possam ser relevantes na busca, no meio de palavras derivadas. O stemming é aplicado sobre as palavras encontradas em um texto e elas são referenciadas como seu vocabulário de palavras. Um exemplo disso é a palavra casarão em um texto.\n",
    "    \n",
    "O stemming de casarão consiste na retirada do sufixo -rão e a palavra vira casa, facilitando assim a busca de palavras\n",
    "Podendo aparecer numa solicitação de busca essa palavra, quando o usuário decide pesquisar a palavra casa</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Bibliotecas externas utilizadas </b></h3> Fonte: https://docs.python.org/3/library/\n",
    "\n",
    "<ul>\n",
    "    <li>os : permite utilizar funcionalidades dependentes do sistema operacional.</li>\n",
    "    <li>typing : fornece uma maneira de sugerir tipos para ajudar verificadores de tipo estático e linters a prever erros com precisão.</li>\n",
    "    <li>functools : faz parte dos Functional Programming Modules e serve para funções que atuam ou retornam outras funções. Qualquer objeto que pode ser chamado pode ser tratado como uma função para os fins deste módulo.</li>    \n",
    "    <li>collections : implementa tipos de dados de contêiner especializados. Alternativas para dict, list, set e tuple.</li>    \n",
    "    <li>abc : Abstract Base Classes. Fornece a infraestrutura para definir classes básicas abstratas (ABCs) em Python, a metaclasse ABCMeta para definir ABCs e uma classe auxiliar ABC para definir ABCs por meio de herança:</li>    \n",
    "    <li>string : operações de string comuns</li>    \n",
    "    <li>gc : Garbage Collector interface. Permite desabilitar o Garbage Collector, ajustar frequência de coleta e definir opções de depuração, além de fornecer acesso a objetos inacessíveis que o coletor encontrou mas não pode liberar.</li> \n",
    "    <li>IPython.display : API pública para ferramentas de exibição no IPython (ferramentas para computação interativa e paralela em Python).</li> \n",
    "    <li>bs4 : Beautiful Soup permmite extrair dados de arquivos HTML e XML.</li> \n",
    "    <li>nltk: The Natural Language Toolkit (NLTK) é uma biblioteca Python oper source para processamento de linguagem natural.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>REFERÊNCIAS BIBLIOGRÁFICAS</h2>\n",
    "\n",
    "<ul>[1] Elmasri, R.; Navathe, S. B. Sistemas de Banco de Dados.  Editora : Pearson Universidades; 6ª edição, 22 dezembro 2010.</ul>\n",
    "<ul>[2] BAEZA-YATES, R.; RIBEIRO-NETO, B. Modern information retrieval.</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
